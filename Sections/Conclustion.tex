\section{Conclusion}
\label{sec:conslusion}

In this study we have compared a variety of different state-of-the-art bioacoustic deep learning models, representing different training paradigms and training domains.
To compare the models, we have isolated their feature extractors and used them to generate embeddings of a curated evaluation dataset consisting of annotated bird sounds from recordings in Costa Rica and Colombia.
The aim of this study was to firstly present a large comparison of very different bioacoustic deep learning feature extractors and to evaluate how training paradigms and training domains affect performance.
Performance was evaluated through clustering using an AMI score and through linear classification using a macro accuracy score.

We have shown that in spite of recent improvements in bioacoustic self-supervised learning, performance is still inferior to that of supervised learning models.
This performance difference is visible in both linear classification and even more in clustering.
Furthermore, we have shown that alignment of training domain and target domain during pretraining impacts performance more, than during fine-tuning.
This study presents a roadmap for a more in-depth performance evaluation of bioacoustic deep learning models, allowing for a better understanding of how training setup impacts downstream performance.