\section{Conclusion}
\label{sec:conslusion}

In this study we have compared a variety of different state-of-the-art bioacoustic deep learning models, representing different training paradigms and training domains.
To compare the models, we have isolated their feature extractors and used them to generate embeddings of two curated evaluation datasets consisting of annotated bird and frog sounds.
The aim of this study was to firstly present a large comparison of very different bioacoustic deep learning feature extractors and to evaluate how training setup affects performance.
Performance was evaluated through clustering using an AMI score and through kNN classification using a macro accuracy score.

We have shown that bioacoustic feature extractors still struggle with polyphonic PAM datasets, especially if they are outside of the training domain.
% At this point, self-supervised learning performance is still inferior to that of supervised learning models.
However, when comparing training paradigms,  supervised learning remains a strong method to pretrain a feature extractor for general use, despite advances in self-supervised learning.
This performance difference is visible in both kNN classification and even more in clustering.
Furthermore, we have shown that alignment of training domain and target domain during pretraining impacts performance more than during fine-tuning.
This study presents a roadmap for a more in-depth performance evaluation of bioacoustic deep learning models, allowing for a better understanding of how training setup impacts downstream performance.