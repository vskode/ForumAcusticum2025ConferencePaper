\section{Discussion}
\label{sec:discussion}

Although the self-supervised feature extractors represented in this study are trained on very large datasets, their combined performance is inferior to most of the supervised learning feature extractors including those trained on non-bird categories.
Perch and BirdNET \cite{kahl_birdnet_2021}, both of which are trained on thousands of classes of bird vocalizations vastly outperform the other feature extractors. 
It is worth mentioning that they do so, while being trained on standard EfiicienNETs.
Similarly, SurfPerch \cite{williams_leveraging_2024} and AvesEcho\_PaSST \cite{ghani_generalization_2024}, both of which have largely benefited from Perch or BirdNET pretraining, perform well by both clustering and classification.
More generally, the embedding spaces of feature extractors trained on very large annotated bird song datasets seem to yield good separation between clusters.

% Interestingly, Insetct66 and Insect459 both of which were trained on only insect sounds and far fewer classes than the bird-trained biolingual, vastly outperform it in classification.
As stated in the introduction, self-supervised learning models lack supervision and might therefore learn classes not meaningful to differentiate between species vocalizations.
When comparing Figures \ref{fig:embeds} and \ref{fig:orig_vs_ump} we observe poor clustering both in terms of AMI performance and by qualitative visual analysis of the embedding separation, which could be resulting from non-meaningful classes.
However, for the three AVES feature extractors, the classifier is nontheless able to learn a meaningful differentiation between the classes.
This could be attributed to the fact, that while they are self-supervised, the training data consisted of curated and non-sparse sound events, thereby increasing the likelihood that meaningful classes are learned.
The drastic performance increase of all three AVES models when evaluated with the frog dataset highlights that in this case the self-supervision enabled the extraction of meaningful features despite the domain shift.

For the self-supervised learning feature extractors, fine-tuning seems to only marginally improve performance.
The three feature extractors based on the AVES models all share the same general audio pretraining and architecture but differ largely in fine-tuning. 
The similarity in performance indicates that the dominant influence on the structuring of embeddings is defined by either the pretraining or the architecture.
Animal2vec\_XC and Animal2vec\_MK, the latter of which is fine-tuned, largely share the same architecture but were trained on very different datasets.
Yet, both models reach similarly bad performance, this is especially surprising for the fine-tuned Animal2vec\_MK.
For the supervised learning feature extractor SurfPerch, which was developed for marine data in coral reefs, bird training data from Perch was mixed with coral reef sounds during pretraining.
While the target domain is very different from the bird dataset, SurfPerch still reaches very high performance.
This performance drops in terms of classification though, as soon as the target domain is shifted to the frog dataset.
Again, this indicates that including data of the target domain is more effective during pretraining than fine-tuning.

While the dimensions of the feature extractors vary greatly, performance does not correlate with dimension.
Nonetheless, in Tab. \ref{tab:results} we demonstrated that using a standardized embedding space affects clustering and classification performance differently.
The performance differences that can be observed are predominantly in clustering, indicating that the graph structure, which KMeans builds for the clustering, is aided by dimensionality reduction using UMAP.
For this study dimensionality reduction using Principal Component Analysis was also performed and evaluated using linear classification, however, performance only changed marginally and was therefore omitted from this comparison.

This analysis underlines the high quality of embeddings created by large supervised learning feature extractors like BirdNET and Perch.
While their training domain aligned with the domain bird dataset, so did that of biolingual, Animal2vec\_XC and BirdAVES\_ESpecies, none of which reached performance metrics similar to BirdNET and Perch.
The dramatic decrease in classification performance by Perch raises the question if the fact that Perch's training data is comprised of solely xeno-canto recordings makes it less domain agnostic than BirdNET which was trained on selected curated bird datasets along with a large portion of xeno-canto.

This study is meant to present a workflow for a more in-depth analysis of embedding spaces, which can be reproduced with the provided repository bacpipe\footnote{\url{github.com/bioacoustic-ai/bacpipe}}.
Evaluation through clustering and classification has shown to vary significantly when applied to the different evaluation sets, undermining the use of both metrics to better understand how training setup affects performance.
By establishing a default analyses of feature extractors alongside the common classification benchmarks, bioacoustic research can accelerate towards a better understanding of what training characteristics are beneficial in this domain.