\section{Discussion}
\label{sec:discussion}

Although the self-supervised feature extractors represented in this study are trained on very large datasets, their clustering performance is inferior to most of the supervised learning feature extractors.
Nonetheless, their lack of supervision demonstrates generalization as the majority of self-supervised feature extractors improve performance on the frog data, both for clustering and classification.
Yet, as has been shown in previous studies \cite{ghani_global_2023}, supervised learning and training data consisting of large bird song databases yield the best results, even for challenging PAM datasets.
Perch and BirdNET \cite{kahl_birdnet_2021}, both of which are trained on thousands of classes of bird vocalizations, vastly outperform the other feature extractors by clustering and for the bird data also by classification. 
%It is worth mentioning that they do so, while being trained on standard EfficienNETs, rather than custom architectures developed for a specific task.
Despite the authors claims, the multi-modal feature extractor Biolingual, which performs very well by clustering but comparably poor by classification is outperformed by BirdNET and Perch.
SurfPerch \cite{williams_leveraging_2024} and AvesEcho\_PaSST \cite{ghani_generalization_2024}, both of which have largely benefited from Perch or BirdNET pretraining, perform well for both clustering and classification tasks.
Most notably AvesEcho\_PaSST is the only supervised learning feature extractor that demonstrates generalization by improving performance on the frog data.
This could be attributed to the models' innovative transformer architecture using patchout, thereby selectively dropping patches of spectrograms during training, whilst having learned from the well trained BirdNET embeddings through knowledge distillation \cite{ghani_generalization_2024}.


% Interestingly, Insetct66 and Insect459 both of which were trained on only insect sounds and far fewer classes than the bird-trained biolingual, vastly outperform it in classification.
As stated in the introduction, self-supervised learning models lack supervision and might therefore learn classes not meaningful to differentiate between species vocalizations.
When comparing Figures \ref{fig:embeds} and \ref{fig:orig_vs_ump} we observe poor clustering both in terms of AMI performance and by qualitative visual analysis of the embedding separation, which could be resulting from non-meaningful classes.
However, for the three AVES feature extractors, the kNN classifier is nonetheless able to learn a meaningful differentiation between the classes.
This could be attributed to the fact, that while they are self-supervised, the data used to train these models consisted of curated and non-sparse sound events, thereby increasing the likelihood that meaningful classes are learned.
Furthermore, the HuBERT architecture used for the AVES models uses tokenization through acoustic unit discovery \cite{hagiwara_aves_2022}.
These acoustic units are generated during training based on 39-dimensional Mel-frequency ceptral coefficients (MFCC) features and labeled using a K-Means clustering.
The transformer encoder then predicts these cluster labels.
This differs fundamentally from the training procedure of the Animal2vec models and AudioMAE, both of which rely on masked prediction.
In terms of classification, the results suggest that the MFCC features lead to more meaningful classes than the prediction of masked emebddings.
This is supported by the drastic performance increase of all three AVES models when evaluated for the frog dataset.

For the self-supervised learning feature extractors, fine-tuning seems to only marginally improve performance.
The three feature extractors based on the AVES models all share the same general audio pretraining and architecture but differ largely in fine-tuning. 
The similarity in performance indicates that the dominant influence on the structuring of embeddings is defined by either the pretraining or the architecture.
Animal2vec\_XC and Animal2vec\_MK, the latter of which is fine-tuned, largely share the same architecture but were trained on very different datasets.
Yet, both models reach similarly poor performance, this is especially surprising for the fine-tuned Animal2vec\_MK.

%For the supervised learning feature extractor SurfPerch, which was developed for marine data in coral reefs, bird training data from Perch was mixed with coral reef sounds during pretraining.
%While the target domain is very different from the bird dataset, SurfPerch still reaches very high performance.
%This performance drops in terms of classification though, as soon as the target domain is shifted to the frog dataset.
%Again, this indicates that including data of the target domain is more effective during pretraining than fine-tuning.


While the dimensions of the feature extractors vary greatly, performance does not correlate with dimensionality.
Nonetheless, in Table \ref{tab:results} we demonstrated that using a standardized embedding space affects clustering and classification performance differently.
% The performance differences that can be observed are predominantly in clustering, indicating that the graph structure, which K-Means builds for the clustering, is aided by dimensionality reduction using UMAP.
% For this study dimensionality reduction using Principal Component Analysis was also performed and evaluated using linear classification, however, performance only changed marginally and was therefore omitted from this comparison.


% This analysis underlines the high quality of embeddings created by large supervised learning feature extractors like BirdNET and Perch.
While the bird data was in-domain for BirdNET and Perch, the same applies for Biolingual, Animal2vec\_XC and BirdAVES\_ESpecies, none of which reached similar performance metrics.
The decrease in classification performance by Perch on the frog data raises the question if this difference can be attributed to its training dataset.
Perch's training data, which is comprised of solely xeno-canto recordings perhaps makes it less domain agnostic than BirdNET which was trained on selected curated bird datasets along with a large portion of xeno-canto.


This study presents a workflow for in-depth analysis of embedding spaces, which can be reproduced with the provided repository bacpipe.
Evaluation through clustering and classification has shown to vary significantly when applied to the different evaluation sets, undermining the use of both metrics to better understand how training setup affects performance.
While the results presented here allow for further discussion, we believe streamlining the process of evaluating and comparing such a wide variety of feature extractors can help to investigate performance differences as a function of training paradigm and training domain.
By establishing a default analysis of feature extractors alongside the common classification benchmarks, bioacoustic research can accelerate towards a better understanding of what training characteristics are beneficial in this domain.