\section{Introduction}
\label{sec:introcution}

Human-driven climate change and deforestation have caused a rapid decline in global biodiversity \cite{butchart_global_2010}.
Using sensor arrays for biodiversity monitoring, ecologists can gather information on environments and investigate how human pressures affect biodiversity and how we can halt its decline \cite{schmeller_building_2017}.
Passive acoustic monitoring (PAM) is one method that provides a low-cost and non-invasive way to monitor biodiversity \cite{sugai_terrestrial_2019}.
The vast amount of data generated by PAM sensors has led to the rapid development of bioacoustic deep learning models to help researchers reduce the annotation effort \cite{stowell_computational_2022}.
The use of these state-of-the-art models has proven valuable in ecological studies, for example in general species assessments \cite{tuia_perspectives_2022,cowans_improving_2024} or detection of endangered species \cite{allen-ankins_use_2025}.

While bioacoustic deep learning models are a useful asset in ecology, it is crucial to understand the model's limitations based on their training setup.
% little is known about how their training setup correlates to their performance.
The two main training strategies to develop bioacoustic deep learning models are supervised learning and self-supervised learning.
% In supervised learning large acoustic datasets are collected, annotated and used to train \cite{kahl_birdnet_2021}. 
Supervised learning models require large amounts of annotated data to be trained \cite{hagiwara_aves_2022}.
The models classify sounds based on a fixed number of predefined classes representing annotated sounds in the dataset.
While supervised learning can have the benefit of instructing models to differentiate between species vocalizations, it requires annotated datasets.
This limits supervised learning models to known and annotated classes and makes them sensitive to class imbalance and label quality.
% In self-supervised learning classes are defined by the model during training time \cite{baevski_efficient_2023,huang_masked_2022}.
Self-supervised learning can be executed in different ways.
In a paradigm referred to as masked prediction, the model is trained to predict a masked portion of the audio, thereby modelling bioacoustic characteristics without supervision \cite{baevski_efficient_2023,huang_masked_2022}.
With growing annotated databases supervised learning models like Birdnet \cite{kahl_birdnet_2021} improve in performance, yet recent developments in self-supervised learning on models like animal2vec \cite{schafer-zimmermann_animal2vec_2024} indicate a promising new direction for the field requiring less manual annotation.
Not requiring annotations, self-supervised learning models can be trained on far larger datasets, however there is no control of what is being learned.
The model might learn to differentiate between sounds based on very different characteristics than the species that produces them.

% The datasets that are created in this manner tend to feature many annotations for few classes and few annotations for many classes (long-tail distribution/class imbalance) \cite{arnaud_improving_2023}. 
% Models with a heavily imbalanced training dataset have shown to yield performance imbalances \cite{hamer_birb_2023}. 
% This implies that the models results are only reproducible when used to classify the well represented majority classes. 
% Furthermore, models trained in a supervised learning pipeline are restricted to only recognize a fixed set of classes. 
% Class imbalance and closed set recognition are two examples of phenomena, that show while supervised learning is the convention, it has shortcomings that restrict the usability of models trained in this way.

To evaluate bioacoustic deep learning models, a comparison of only the classifier performance obscures the fine-grained differences between models and how they analyze input sounds.
Bioacoustic deep learning models consist of artificial neural networks which can be subdivided into feature extractors and classifiers. 
The feature extractor creates an embedding (vector representation) of an input sound and the classifier (which corresponds to the final dense layer of the model) maps the embedding onto classes.
Commonly, in bioacoustics, a suite of established benchmarks is used to compare the classifier performance of state-of-the-art models \cite{hamer_birb_2023}.
This requires the models to have been trained on the classes present in the benchmarking datasets.
However, there is an alternative: using the embeddings created by the feature extractors, the generated embedding spaces can be analyzed in regard to their structural characteristics, irrespective of what the classifiers were trained on.

Output dimensions of feature extractors vary greatly, but it is uncertain if their dimensionality correlates to the downstream classifier performance. 
Dimensionality reduction algorithms are useful to standardize the dimensionality of different feature extractors, as well as help visualize the high dimensional embedding spaces.
% Due to the lack of separable clusters in ecoacoustics and soundscape analyzes, evaluation of embedding spaces (reduced to two dimensions) has become common in recent years \cite{sethi_characterizing_2020,calonge_revised_2024,parcerisas_categorizing_2023}. 
However, there is little investigation of how reducing the embedding space affects performance.
We therefore compare performance in both original and reduced embedding spaces.

Due to their size, datasets that are used to train bioacoustic models for bird detection are often based on citizen science data (e.g. xeno-canto \cite{xeno-canto_xeno-canto_2025}).
The majority of these recordings are focal recordings of individuals which are weakly labeled with little polyphony.
When models get applied to large PAM datasets, these feature very different recording conditions.
To accurately evaluate the models in this study, we use a PAM bird vocalization dataset from Colombia and Costa Rica \cite{vega-hidalgo_collection_2023}, as well as a dataset of frog vocalizations in tropical rainforests of Brazil \cite{canas_dataset_2023}.
Both datasets are comprised of PAM recordings in noisy environments and can therefore be considered as challenging datasets.
Through the selection of challenging datasets, we hope to on the one hand emphasize the performance differences between the models and on the other hand produce results that will reflect in real world applications.

This study aims to showcase the potential for evaluating and especially comparing bioacoustic deep learning models in regard to their training paradigm and training data.
This evaluation is based on the structuring capabilities of their respective feature extractors analyzed through clustering and classification performance.
Classification in this case refers to recognition of novel classes, as none of the models have been pretrained on the classes selected here.
This way single classification layers are attached to each feature extractor, all of which are trained on the same evaluation sets (bird and frog vocalizations), i.e. same classes and same data, allowing us to compare their performance.
Classification is done both in a linear and a k-nearest neighbor (kNN) approach.
We perform our analysis in both the original embedding space and a reduced dimensional embedding space. 
That way we ensure the dimension is standardized for the second evaluation, whilst we can investigate performance differences between the original embedding space and the reduced space.
% This study will therefore examine how different dimensionality reduction strategies impact performance. 
% While the possibilities of analysis of these high dimensional embedding spaces are vast, we limit ourselves to 
% Performance will be evaluated through an analysis of clustering based on
% \begin{enumerate}
%     \item Silhouette Score \cite{rousseeuw_silhouettes_1987}, 
%     \item Adjusted Rand Index \cite{steinley_variance_2016} based on KMeans clustering and
%     \item Adjusted Mutual Information \cite{romano_standardized_2014} based on KMeans clustering.
% \end{enumerate}
% To do so, we analyse clusterings of deep learning feature extractors and use them to compare different training setups.
This method of analysis opens up the possibilities for a fair comparison of deep learning feature extractors guiding the field to a better understanding how training configurations affect downstream performance.
% Summary of what we are contributing.




