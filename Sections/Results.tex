\section{Results}
\label{sec:results}

\begin{figure*}[ht]
    \centerline{\framebox{
    \includegraphics[width=16.3cm]{Sections/imgs/normal_overview.png}}}
    \caption{Two-dimensional embedding spaces of all feature extractors, sorted descending by their clustering performance of AMI values (indicated next to their name) from top left to bottom right.
    % Given the different input lengths of the feature extractors, the number of embeddings vary significantly.
    Colors correspond to the class labels, which are 11 different tropical bird species.}
    \label{fig:embeds}
\end{figure*}

% Embeddings spaces of are visualized using UMAP in are generated from the input data using all feature extractors and then the dimensionality reduction algorithm UMAP is used to visualize the data in two dimensions.
% The results are shown in Fig. \ref{fig:embeds}.

Two-dimensional UMAP embeddings are shown in Fig.~\ref{fig:embeds}.
The worst performing feature extractors, produce large unstructured clouds of mixed color, indicating that no significant clustering is achieved.
In the first and second row, feature extractors can be seen to separate the embeddings into meaningful clusters.
It is noticeable that some feature extractors such as AvesEcho\_PaSST and ProtoCLR seem to generate more subclusters than most other feature extractors.
The seven best performing feature extractors are all trained using supervised learning and the top three additionally trained on bird vocalizations.
All three of the AVES models (BirdAVES, AVES and NonBioAVES) reach similar performances in spite of big differences in their fine-tuning datasets \cite{hagiwara_aves_2022}.


\begin{figure}[ht]
    \centerline{\framebox{
    \includegraphics[width=7.8cm]{Sections/imgs/scatterplot_clust_vs_class_neotrop_anuran_normal.png}}}
    \caption{Comparison of feature extractor performance by learning paradigm, training data and application data. 
    Abbreviated names correspond to abbrev. column in Tab.~\ref{tab:bacpipe_models}. 
    The x-axis shows clustering results of AMI while the y-axis shows macro accuracy results of kNN classification. 
    Colors correspond to supervised learning and self-supervised learning feature extractors, while symbols separate bird and non-bird training data.
    Black points show the performance of each feature extractor on the frog dataset.
    Red and blue points show the performance on the bird dataset. 
    The gray arrow shows the performance change from the bird to the frog dataset.}
    \label{fig:orig_vs_ump}
\end{figure}
    

To highlight performance changes once the feature extractors are applied to a dataset different from their training domain, Fig. \ref{fig:orig_vs_ump} shows the performance on the bird dataset (black) and the frog dataset (red and blue) connected by an arrow.
Performance is evaluated by macro accuracy of knn classification on the x-axis and AMI of clustering on the y-axis.

First we will focus on the feature extractors being applied to the bird dataset, shown in red and blue.
When focussing on the y-axis, all self-supervised learning feature extractors (in red) reach clustering performances under 0.31 while the 6 best performing feature extractors are trained using supervised learning.
Performance by kNN classification is more equally distributed, however, again supervised learning feature extractors reach the three highest values.
Furthermore, Animal2vec\_XC, the only self-supervised learning feature extractor that was not fine-tuned, performs poorly by both clustering and kNN classification.
Google\_Whale represents the only supervised learning feature extractor performing very poorly by clustering and kNN classification.

Comparing by training data, feature extractors trained on only or including bird datasets outperform the other feature extractors by kNN classification and even more so by clustering.
Aside from ProtoCLR the supervised learning models that are also trained on birds vastly outperform all other models in the combination of clustering and kNN classification.
Perch and BirdNET lead in both clustering and kNN classification by a large margin.
Biolingual, which was trained on large bird databases using a multi-modal approach performs well by clustering, but comparatively poorly by kNN classification.

The arrows show the performance change when the feature extractors are applied to the frog dataset.
All self-supervised feature extractors improve in clustering performance and aside from Animal2vec\_MK also in kNN classification.
The self-supervised AVES feature extractors (BirdAVES, AVES and NonBioAVES) drastically improve in both clustering and kNN classification on the frog dataset.
So much so, that all three outperform all other feature extractors by kNN classification.
% Aside from all feature extractors increase in clustering performance.
Among supervised learning feature extractors, changes in both clustering and kNN classification performance are more varied.
Especially the three best performing models by clustering for the bird dataset, Perch, BirdNET and Biolingual, all decrease in performance when applied to the frog dataset.

All non-bird trained feature extractors improve in clustering.
From the supervised bird trained feature extractors, only AvesEcho improves in both clustering and kNN classification.
Again, when applied to the frog dataset, all bird trained feature extractors except for Animal2vec\_XC outperform the rest by clustering.
The top 6 models by clustering are again the supervised learning bird trained feature extractors.
However, by kNN classification, results are more mixed by both training paradigm and training domain.

When referring back to Table \ref{tab:bacpipe_models} embedding dimension does not correlate with clustering or kNN classification performance.
Furthermore, the only two feature extractors trained on marine sounds, Google\_Whale and SurfPerch (trained on birds and marine sounds) reach very different performances.
    

\input{Sections/table_res}

Table \ref{tab:results} shows the averaged performances over all feature extractors in the different categories: supervised and self-supervised learning and bird and non-bird.
Firstly we want to point out the performance changes following dimensionality reduction using UMAP.
For evaluation with both bird and frog datasets, UMAP embeddings improve the clustering results significantly for every category.
For the classification performance values are very similar between original and UMAP reduced embeddings, however, while for the bird dataset UMAP yields to performance increases for most categories, for the frog dataset, the original embeddings yield better restults.
In line with Fig. \ref{fig:orig_vs_ump}, supervised learning outperform the self-supervised learning feature extractors by a large margin for classification and clustering in the bird dataset.
For the frog dataset, due the the improved performance of the AVES models, self-supervision outperforms supervised learning by kNN classification.
When comparing the values between all categories, the bird trained feature extractors outperform all other categories for both evaluation sets.
