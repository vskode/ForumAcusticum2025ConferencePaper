\section{Introduction}
\label{sec:introcution}

Human-driven climate change and deforestation have caused a rapid decline in global biodiversity \cite{butchart_global_2010}.
Using sensor arrays for biodiversity monitoring, ecologists can gather information on environments and investigate how human pressures affect biodiversity and how we can halt its decline \cite{schmeller_building_2017}.
Passive acoustic monitoring (PAM) is one method that provides a low-cost and non-invasive way to monitor biodiversity \cite{sugai_terrestrial_2019}.
The vast amount of data generated by PAM sensors has lead to the rapid development of bioacoustic deep learning models to help researchers reduce the annotation effort \cite{stowell_computational_2022}.
The use of these state-of-the-art models has proven valuable in ecological studies, for example in general species assessments \cite{tuia_perspectives_2022,cowans_improving_2024} or detection of endangered species \cite{allen-ankins_use_2025}.

While bioacoustic deep learning models are a useful asset in ecology, it is crucial to understand the model's limitations based on their training setup.
% little is known about how their training setup correlates to their performance.
The supervised learning paradigm has emerged as the dominant training pipeline for deep learning models in bioacoustics, in which large acoustic datasets are collected, annotated and used to train deep learning models \cite{allen_convolutional_2021,aodha_bat_2018,bergler_orca-spot_2019,kahl_birdnet_2021}. 
During training, the models classify sounds based on a fixed number of predefined classes representing annotated sounds in the dataset.
Self-supervised learning is an alternative paradigm, in which classes are defined by the model during training time \cite{baevski_efficient_2023,huang_masked_2022}.
While supervised learning can have the benefit of instructing models to differentiate between different species vocalizations, self-supervised learning does not require annotated datasets.
This limits supervised learning models to known and annotated classes and makes them sensitive to class imbalance and label quality.
Not requiring annotations, self-supervised learning models can be trained on far larger datasets, however the sparsity of bioacoustic events can lead to non-meaningful classes.
With growing annotated databases supervised learning models like Birdnet \cite{kahl_birdnet_2021} improve in performance, yet recent developments in self-supervised learning on models like animal2vec \cite{schafer-zimmermann_animal2vec_2024} indicate a promising new direction for the field requiring less manual annotation.

% The datasets that are created in this manner tend to feature many annotations for few classes and few annotations for many classes (long-tail distribution/class imbalance) \cite{arnaud_improving_2023}. 
% Models with a heavily imbalanced training dataset have shown to yield performance imbalances \cite{hamer_birb_2023}. 
% This implies that the models results are only reproducible when used to classify the well represented majority classes. 
% Furthermore, models trained in a supervised learning pipeline are restricted to only recognize a fixed set of classes. 
% Class imbalance and closed set recognition are two examples of phenomena, that show while supervised learning is the convention, it has shortcomings that restrict the usability of models trained in this way.

To evaluate bioacoustic deep learning models, a comparison of only the classifier performance obscures the fine-grained differences between models and how they analyse input sounds.
Bioacoustic deep learning models consist of artificial neural networks which can be subdivided into feature extractors and classifiers. 
The feature extractor creates an embedding (vector representation) of an input sound and the classifier (which corresponds to the final dense layer of the model) maps the embedding onto classes.
In the case of supervised learning these classes are predefined.
In self-supervised learning these classes are learned during training.
Commonly, in bioacoustics, a suite of established benchmarks is used to compare the classifier performance of state-of-the-art models \cite{hamer_birb_2023,canas_dataset_2023,calonge_revised_2024}.
This requires the models to have been trained on the classes present in the benchmarking datasets or to be retrained on a standardized training dataset to enable a comparison.
However, using the embeddings created by the feature extractors, the generated embedding spaces can be analysed in regard to their structural characteristics, irrespective of the classes the classifiers were trained on.

This study aims to showcase the potential for evaluation and especially comparison of bioacoustic deep learning models based on the structuring capabilities of their respective feature extractors.
Output dimensions of feature extractors vary greatly, but it is uncertain if their dimensionality correlates to the downstream classifier performance. 
This study will therefore examine how different dimensionality reduction strategies impact performance. 
% While the possibilities of analysis of these high dimensional embedding spaces are vast, we limit ourselves to 
% Performance will be evaluated through an analysis of clustering based on
% \begin{enumerate}
%     \item Silhouette Score \cite{rousseeuw_silhouettes_1987}, 
%     \item Adjusted Rand Index \cite{steinley_variance_2016} based on KMeans clustering and
%     \item Adjusted Mutual Information \cite{romano_standardized_2014} based on KMeans clustering.
% \end{enumerate}
Due to the lack of separable clusters in ecoacoustics and soundscape analyses, evaluation of embedding spaces in that research field has become common in recent years \cite{sethi_characterizing_2020,calonge_revised_2024,parcerisas_categorizing_2023}. 
However, there is no standardized practice of quantitatively assessing the clustering capabilities of deep learning feature extractors. 
This analysis is a proof of concept of the capabilities of quantifying feature extractor performance and considered a preliminary study for more in-depth analyses of embedding spaces. 


% Summary of what we are contributing.




